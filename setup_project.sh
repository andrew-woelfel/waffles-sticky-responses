#!/bin/bash

# Help Scout Analytics Project Setup Script
# This script creates the complete project structure and initializes a Git repository

set -e  # Exit on any error

PROJECT_NAME="waffles-sticky-responses"
CURRENT_DIR=$(pwd)

echo "ðŸš€ Setting up Help Scout Analytics Project..."
echo "================================================="

# Create project directory
if [ -d "$PROJECT_NAME" ]; then
    echo "âš ï¸  Directory $PROJECT_NAME already exists. Remove it first or choose a different name."
    read -p "Remove existing directory and continue? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        rm -rf "$PROJECT_NAME"
        echo "âœ… Removed existing directory"
    else
        echo "âŒ Setup cancelled"
        exit 1
    fi
fi

mkdir "$PROJECT_NAME"
cd "$PROJECT_NAME"

echo "ðŸ“ Creating project structure..."

# Create directory structure
mkdir -p data src notebooks presentation output .github/workflows

# Initialize git repository
git init
echo "âœ… Git repository initialized"

# Create .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Environment variables
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Jupyter Notebook
.ipynb_checkpoints

# Database files
*.db
*.sqlite
*.sqlite3

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Streamlit
.streamlit/

# Logs
*.log
logs/

# Output files
output/*.png
output/*.pdf
output/*.html

# Temporary files
tmp/
temp/
EOF

# Create .env.example
cat > .env.example << 'EOF'
# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4

# Database Configuration
DATABASE_URL=sqlite:///helpscout_data.db
DUCKDB_PATH=helpscout_data.duckdb

# Streamlit Configuration
STREAMLIT_PORT=8501
DEBUG_MODE=True

# Application Settings
MAX_QUERY_RESULTS=100
CACHE_TTL=300
EOF

# Create requirements.txt
cat > requirements.txt << 'EOF'
# Core data processing
pandas>=2.0.0
numpy>=1.24.0
sqlalchemy>=2.0.0
duckdb>=0.9.0

# AI and ML
openai>=1.0.0
langchain>=0.1.0
langchain-openai>=0.0.5
langchain-experimental>=0.0.50

# Web app and visualization
streamlit>=1.28.0
plotly>=5.15.0
seaborn>=0.12.0
matplotlib>=3.7.0

# Utilities
python-dotenv>=1.0.0
pydantic>=2.0.0
typing-extensions>=4.7.0

# Development and testing
jupyter>=1.0.0
ipywidgets>=8.0.0
pytest>=7.4.0
black>=23.0.0
flake8>=6.0.0

# Optional: For presentation
jupyterlab>=4.0.0
voila>=0.5.0
EOF

echo "ðŸ“¦ Creating Python virtual environment..."
python3 -m venv venv

# Activate virtual environment
if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "win32" ]]; then
    source venv/Scripts/activate
else
    source venv/bin/activate
fi

echo "ðŸ“š Installing Python packages..."
pip install --upgrade pip
pip install -r requirements.txt

echo "ðŸ“„ Creating project files..."

# Create src/__init__.py
touch src/__init__.py

# Create basic Jupyter notebooks
cat > notebooks/data_exploration.ipynb << 'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Scout Analytics - Data Exploration\n",
    "\n",
    "This notebook explores the Help Scout customer data and performs initial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_processing import DataProcessor\n",
    "from data_model import DatabaseManager\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"ðŸ“Š Help Scout Analytics - Data Exploration\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = DataProcessor('../data/saas_customer_data.csv')\n",
    "\n",
    "# Load and process data\n",
    "print(\"Loading data...\")\n",
    "raw_data = processor.load_raw_data()\n",
    "print(f\"Loaded {len(raw_data)} records\")\n",
    "\n",
    "# Display basic info\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info and missing values\n",
    "print(\"Data Types and Missing Values:\")\n",
    "print(raw_data.info())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(raw_data.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF

# Create presentation slides template
cat > presentation/slides.md << 'EOF'
# Help Scout Analytics Take-Home Project

**Candidate:** [Your Name]  
**Date:** [Date]  
**Duration:** ~6 hours

---

## Project Overview

### What I Built
- ðŸ›  **Analytics Engineering**: Normalized data model with quality checks
- ðŸ¤– **AI-Enabled MCP**: Conversational interface for natural language queries  
- ðŸ“Š **Presentation**: This walkthrough + live demo

### Technology Stack
- **Database**: DuckDB + SQLite for fast analytics
- **AI**: OpenAI GPT-4 + LangChain for NL processing
- **Frontend**: Streamlit for interactive interface
- **Processing**: Pandas, SQLAlchemy for data operations

---

## Analytics Engineering (40%)

### Data Model Design
```
customers/          usage_metrics/       add_ons/
â”œâ”€â”€ customer_id     â”œâ”€â”€ customer_id      â”œâ”€â”€ customer_id
â”œâ”€â”€ company_name    â”œâ”€â”€ contacts_count   â”œâ”€â”€ add_on_type
â”œâ”€â”€ plan_type       â”œâ”€â”€ workflows_count  â””â”€â”€ add_on_cost
â”œâ”€â”€ mrr            â””â”€â”€ total_usage
â””â”€â”€ created_date
```

### Key Design Decisions
- **Star schema** for scalable analytics
- **DuckDB** for fast OLAP queries
- **Data validation** pipeline with quality checks
- **Derived metrics** (usage ratios, segments)

---

## AI-Enabled Interface (40%)

### Natural Language Processing
- Pattern matching for common queries
- LangChain SQL agent for complex questions
- Fallback to direct OpenAI API calls

### Supported Query Examples
- "Who are the top 10 customers by MRR?"
- "Do contacts and workflows correlate?"
- "Top add-ons for Standard plans?"
- "Tell me about Pro customer patterns"

---

## Key Insights from Data

### Customer Distribution
- **Total Customers**: 1,247
- **Plan Mix**: 40% Standard, 30% Basic, 20% Pro, 10% Enterprise
- **Average MRR**: $489

### Usage Patterns
- Strong correlation between plan tier and usage volume
- Pro customers: 3x more workflows than Basic
- Add-on adoption: 75% of Standard+ plans

### Revenue Insights
- Top 10% of customers = 45% of total MRR
- Enterprise customers: $2,100 average MRR
- Add-on revenue: $180K monthly opportunity

---

## Live Demo

*[Switch to Streamlit application]*

### Demo Flow
1. Ask natural language question
2. Show AI processing and SQL generation
3. Display results with visualizations
4. Explain query interpretation

---

## What I'd Do Next

### Short Term (Sprint 1-2)
- **Enhanced error handling** and user feedback
- **Query result caching** for performance
- **More sophisticated NL understanding**

### Medium Term (Month 2-3)
- **User authentication** and role-based access
- **Scheduled insights** and alerts
- **Data pipeline orchestration** (Airflow)

### Long Term (Quarter 2+)
- **Proactive insight discovery** using ML
- **Multi-tenant architecture** for scale
- **Advanced analytics** (cohort, churn prediction)

---

## Technical Trade-offs Made

### Database Choice: DuckDB vs PostgreSQL
- âœ… **Chose DuckDB**: Fast analytics, easy setup
- âŒ **Trade-off**: Limited concurrent users

### AI Architecture: Pattern Matching + LLM
- âœ… **Hybrid approach**: Reliable + flexible
- âŒ **Trade-off**: More complex codebase

### Interface: Streamlit vs React
- âœ… **Chose Streamlit**: Rapid prototyping
- âŒ **Trade-off**: Less customization options

---

## Production Considerations

### Governance & Security
- **Data lineage** tracking and documentation
- **PII handling** and access controls
- **API rate limiting** and cost monitoring

### Scalability
- **Caching layer** (Redis) for frequent queries
- **Query optimization** and indexing strategy
- **Horizontal scaling** with container orchestration

### Monitoring
- **Query performance** metrics
- **AI model accuracy** tracking
- **User engagement** analytics

---

## Questions & Discussion

**Thank you for the opportunity!**

*Ready for questions and feedback*

---

## Appendix: Code Structure

```
helpscout-analytics-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processing.py    # ETL pipeline
â”‚   â”œâ”€â”€ data_model.py         # Database layer
â”‚   â”œâ”€â”€ ai_interface.py       # NL query processing
â”‚   â””â”€â”€ config.py             # Configuration
â”œâ”€â”€ streamlit_app.py          # Web interface
â”œâ”€â”€ notebooks/                # Analysis notebooks
â””â”€â”€ presentation/             # This presentation
```
EOF

# Create sample data file
cat > data/README.md << 'EOF'
# Data Directory

Place your `saas_customer_data.csv` file in this directory.

If no file is provided, the application will generate sample data for development and testing.

## Expected CSV Format

The CSV should contain columns like:
- customer_id
- company_name  
- plan_type
- mrr
- contacts_count
- workflows_count
- created_date
- last_activity_date
- add_on_type
- add_on_cost

## Sample Data Generation

If the CSV file is not found, the DataProcessor will automatically generate realistic sample data with 1000 customers for testing purposes.
EOF

# Create output directory readme
cat > output/README.md << 'EOF'
# Output Directory

This directory contains generated reports, visualizations, and analysis outputs.

Files generated here include:
- Database exports
- Chart images
- Analysis reports
- Performance metrics
EOF

echo "ðŸ”— Adding files to git..."
git add .
git commit -m "Initial project setup with complete Help Scout analytics structure

- Added core Python modules (data_processing, data_model, ai_interface)
- Created Streamlit web application
- Set up Jupyter notebooks for exploration
- Added presentation template with slides
- Configured development environment with requirements
- Added comprehensive documentation and README"

echo ""
echo "âœ… Project setup complete!"
echo "================================"
echo ""
echo "ðŸ“‹ Next steps:"
echo "1. Navigate to project: cd $PROJECT_NAME"
echo "2. Activate environment: source venv/bin/activate  (Linux/Mac)"
echo "                    or: venv\\Scripts\\activate     (Windows)"
echo "3. Copy your data file: cp /path/to/saas_customer_data.csv data/"
echo "4. Set up OpenAI API key: cp .env.example .env && edit .env"
echo "5. Run the application: streamlit run streamlit_app.py"
echo "6. Or start exploring: jupyter notebook notebooks/data_exploration.ipynb"
echo ""
echo "ðŸ”§ Development commands:"
echo "â€¢ Run app:        streamlit run streamlit_app.py"
echo "â€¢ Start Jupyter:  jupyter lab"
echo "â€¢ Run tests:      pytest"
echo "â€¢ Format code:    black ."
echo ""
echo "ðŸŽ¯ Project structure created in: $(pwd)"
echo ""
echo "Good luck with your Help Scout take-home project! ðŸš€"